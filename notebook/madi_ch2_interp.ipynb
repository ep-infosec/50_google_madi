{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAcncYATzpv5"
      },
      "source": [
        "#Chapter 2. Variable Attribution for Multivariate, Multimodal Anomaly Detection with Interpretability\n",
        "\n",
        "For a Detailed discussion please refer to [Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling](https://proceedings.icml.cc/static/paper_files/icml/2020/2557-Paper.pdf) (2020).\n",
        "\n",
        "In practice, knowing which dimensions caused an anomaly score helps the user identify the root cause and choose an appropriate fix. In climate control devices, dimensions like zone temperature, carbon dioxide level, humidity, etc., are codependent. Highlighting a single anomalous dimension\n",
        "usually identifies a broken sensor, whereas identifying multiple dimensions helps pinpoint a mechanical failure caused by a defective valve, stuck damper, etc. As a first step in anomaly interpretation, we would like to quantify a pro-\n",
        "portional blame for each dimension.\n",
        "\n",
        "Recent work on deep network model interpretability can be applied to variable attribution. For example, Integrated Gradients [(Sundararajan et al., 2017](https://arxiv.org/pdf/1703.01365.pdf)), have been shown to indicate what pixels contributed most to an image classification, or what words contributed to a text classification. The Integrated Gradients method computes and integrates the gradient for each dimension from a neutral baseline point to the observed point.\n",
        "\n",
        "In the image classification task, a black image is commonly used as the neutral baseline, and in the text classification task, a zero-embedding token vector is suitable. However, in anomaly interpretation the neutral baseline is not immedi-\n",
        "ately obvious. We fundamentally would like to know, given an anomalous point x, how to transform $x$ into a suitable normal point $u^{∗}$. For example, if the thermostat is generating anomalous data while in eco-mode, we would like to\n",
        "understand which dimensions must be altered to transform the anomalous point to a normal point in eco-mode, rather than in a more remote comfort mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7r_5Ajz8I8U"
      },
      "source": [
        "***Perform variable attribution with Integrated Gradients***, after loading data and training an NS-NN Anomaly Detector, the first step creates sliders for each dimension. The output of that cell should be pasted into the following cell, as marked. Then you can perturb the original Normal state by moving one or more sliders. You will see a pie chart, where the center is the anomaly score $P(x \\in Normal)$ and the wedges of the pie are the blame for each dimension $B_{d}(x)/\\sum B_{d}(x)$ with the observed value $x_{d}$ and the nearest baseline value  $u_{d}^{*}$ for all dimensions $d:B_{d}(x) \\geq 0.05$. The chart below shows individual blames $B_{d}(x)$ vs. $\\alpha \\in [0,1]$ with 2,000 steps from $u^{*}$ to $x$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpJsL1rp4-EL"
      },
      "source": [
        "##Quickstart\n",
        "Simply execute the following cell that performs a pip-install of madi from the github repo.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElkHp0BG4aA9"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/google/madi.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "khx2J8FdP5ug"
      },
      "outputs": [],
      "source": [
        "#@title Colab Imports\n",
        "import sys\n",
        "import madi\n",
        "from madi.utils import file_utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "\n",
        "from madi.datasets import gaussian_mixture_dataset\n",
        "from madi.detectors.neg_sample_neural_net_detector import NegativeSamplingNeuralNetworkAD\n",
        "from madi.detectors.integrated_gradients_interpreter import IntegratedGradientsInterpreter\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.version.VERSION \u003e '2.1.0'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Dg0pQtMGiqwi"
      },
      "outputs": [],
      "source": [
        "#@title Plotting utilities\n",
        "def plot_attribution(df_attribution: pd.DataFrame, anomaly_score: float) -\u003e None:\n",
        "  \"\"\"Plots the attribution as a pie chart.\n",
        "\n",
        "  The center contains the anmomaly score. The wedges are ordered clockwise\n",
        "  from the most blame to the least. The percentages are the normalized\n",
        "  percentages (Blame(d) / Sum of Blames). The values outside the wedges\n",
        "  indicate the observed value, and the expected value in parentheses.\n",
        "\n",
        "  Args:\n",
        "    df_attribution: dataframe with observed_value, expected_value, and\n",
        "    attribution for each dimension.\n",
        "    anomaly_score: score ranging between Normal (1) and Anomalous (0).\n",
        "\n",
        "  \"\"\"\n",
        "  df_attribution = df_attribution.sort_values(by='attribution', ascending=False)\n",
        "  norm = plt.Normalize()\n",
        "  names=[]\n",
        "  sizes = []\n",
        "  sum_big = 0\n",
        "  for fn, row in df_attribution.iterrows():\n",
        "    # Only show the dimensions with a blame \u003e 5%.\n",
        "    if row.attribution \u003e 0.05:\n",
        "      names.append('%s\\n%3.1f (%3.1f)' %(fn, row.observed_value, row.expected_value))\n",
        "      wedge_size = int(100* row.attribution)\n",
        "      sum_big += wedge_size\n",
        "      sizes.append(wedge_size)\n",
        "  names.append('other')\n",
        "  sizes.append(int(100 - sum_big))\n",
        "\n",
        "  # Create a circle for the center of the plot\n",
        "  num_p_score_steps = 100\n",
        "  center_color_index  = int(num_p_score_steps*anomaly_score)\n",
        "  my_circle=plt.Circle( (0,0), 0.45, facecolor=plt.cm.RdYlGn(\n",
        "      norm(range(num_p_score_steps+1)))[center_color_index],\n",
        "      edgecolor='white', linewidth=3)\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  fig.set_figheight(10)\n",
        "  fig.set_figwidth(10)\n",
        "\n",
        "  center_text = \"%.2f\" %(anomaly_score)\n",
        "  if (center_color_index \u003c 20 ) or (center_color_index \u003e 80):\n",
        "    text_color = 'white'\n",
        "  else:\n",
        "    text_color = 'black'\n",
        "  ax.text(0,0,center_text, fontsize=28,horizontalalignment='center',\n",
        "          color=text_color, weight=\"bold\")\n",
        "\n",
        "  # Custom colors --\u003e colors will cycle\n",
        "  norm = plt.Normalize()\n",
        "  # Choose nine colors to cycle through to show contrast between slices.\n",
        "  pie_plot = plt.pie(sizes, labels=names, colors=plt.cm.RdYlBu(norm(range(9)), alpha=0.6),\n",
        "                     startangle=90, counterclock=False, autopct='%1.0f%%',\n",
        "                     pctdistance=0.70, textprops=\n",
        "                     dict(color=\"black\", weight=\"bold\", fontsize=28))\n",
        "\n",
        "  for lab in pie_plot[1]:\n",
        "    lab.set_fontsize(28)\n",
        "  p=plt.gcf()\n",
        "  p.gca().add_artist(my_circle)\n",
        "  plt.show()\n",
        "\n",
        "def plot_gradient_series(df_grad: pd.DataFrame, delta: np.array) -\u003e None:\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_figheight(12)\n",
        "    fig.set_figwidth(15)\n",
        "    n_points = len(df_grad)\n",
        "    colors = sns.color_palette('rainbow', df_grad.shape[1])\n",
        "    for ix, field_name in enumerate(df_grad):\n",
        "      series_color = colors[ix]\n",
        "      ig_series = (df_grad[field_name].cumsum()/float(n_points)) * delta[field_name]\n",
        "      ax.plot(df_grad.index, ig_series , linewidth=3.0, linestyle='-', marker=None, color=series_color)\n",
        "\n",
        "    ax.grid(linestyle='-', linewidth='0.5', color='darkgray')\n",
        "\n",
        "    legend = plt.legend(loc='upper left', shadow=False, fontsize='16', bbox_to_anchor=(1, 1), labels = list(df_grad))\n",
        "    legend.get_frame().set_facecolor('white')\n",
        "    plt.ylabel('Cumulative Gradients')\n",
        "\n",
        "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
        "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "      item.set_fontsize(24)\n",
        "\n",
        "    for sp in ax.spines:\n",
        "       ax.spines[sp].set_color(\"black\")\n",
        "    ax.set_facecolor(\"white\")\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GekUEwUO2RKt"
      },
      "source": [
        "##1. Load up a positive sample dataset.\n",
        "The input data is loaded into a Pandas DataFrame must meet the following criteria:\n",
        "\n",
        "*   Two or more real- or integer-valued feature columns.\n",
        "*   No null values.\n",
        "*   No categorical values.\n",
        "*   A sample size of at least a few thousand rows.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Two example datasets are provided (more coming soon!):\n",
        "\n",
        "\n",
        "A **Gaussian Mixture** with two spherical gaussian modes in 16 dimensions, with 5% contamination.\n",
        "\n",
        "The **Smart Buildings Dataset**.\n",
        "\n",
        "To create your own dataset, simply implement your custom [`madi.datasets.basedataset.BaseDataset`](https://cs.corp.google.com/piper///depot/google3/third_party/py/madi/datasets/base_dataset.py?g=0) and provide a name, description and Pandas DataFrame that meets the criteria above.\n",
        "\n",
        "\n",
        "When everything is loaded, you should see info about the training set size and the test set size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tv3SZUdnu3Fg"
      },
      "outputs": [],
      "source": [
        "#@title Choose the data set\n",
        "_RESOURCE_LOCATION = \"madi.datasets.data\"\n",
        "data_source = \"gaussian_mixture\" #@param [\"gaussian_mixture\", \"smart_buildings\"]\n",
        "ds = None\n",
        "\n",
        "class InvalidDatasetError(ValueError):\n",
        "    pass\n",
        "\n",
        "if data_source == 'gaussian_mixture':\n",
        "  contamination = 0.15\n",
        "\n",
        "  ds = gaussian_mixture_dataset.GaussianMixtureDataset(\n",
        "          n_dim=16,\n",
        "          n_modes=2,\n",
        "          n_pts_pos=8000,\n",
        "          sample_ratio=contamination,\n",
        "          upper_bound=3,\n",
        "          lower_bound=-3)\n",
        "\n",
        "  print('Loaded Gaussian mixture with 2 modes in 16 dimensions, with a sample size of %d.' %len(ds.sample))\n",
        "\n",
        "elif data_source == 'smart_buildings':\n",
        "  data_file = file_utils.PackageResource(\n",
        "      _RESOURCE_LOCATION, \"anomaly_detection_sample_1577622599.csv\")\n",
        "  readme_file = file_utils.PackageResource(\n",
        "      _RESOURCE_LOCATION, \"anomaly_detection_sample_1577622599_README.md\")\n",
        "  ds = madi.datasets.smart_buildings_dataset.SmartBuildingsDataset(data_file, readme_file)\n",
        "  print(ds.description)\n",
        "\n",
        "else:\n",
        "  raise InvalidDatasetError(\"You requested an invalid data set (%s).\" %data_source)\n",
        "\n",
        "\n",
        "print('Randomize the data, and split into training and test sample.')\n",
        "split_ix = int(len(ds.sample) * 0.8)\n",
        "training_sample = ds.sample.iloc[:split_ix]\n",
        "test_sample = ds.sample.iloc[split_ix:]\n",
        "print(\"\\tTraining sample size: %d\" %len(training_sample))\n",
        "print(\"\\tTest sample size: %d\" %len(test_sample))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePrYfXJwCrmz"
      },
      "source": [
        "##2. Train a Negative-Sample Neural Network (NS-NN)\n",
        "Here we set the parameters and train a NS-NN anomaly detector discussed in Section 3.1. The NS-NN first generates a negative sample representing the anomalous class, and produces a labled dataset with the observed sample as the positive dataset representing the normal class. Then it trains a simple binary classifier to distinguish between normal and anomalous classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uIJFPh35S9Hs"
      },
      "outputs": [],
      "source": [
        "#@title Train a Negative Sampling Neural Net (NS-NN) Anomaly Detector\n",
        "log_dir = \"logs/nsnn2\" #@param {type:\"string\"}\n",
        "\n",
        "nsnn_params = {}\n",
        "if data_source == 'gaussian_mixture':\n",
        "\n",
        "    nsnn_params['sample_ratio']=10.0\n",
        "    nsnn_params['sample_delta']=0.05\n",
        "    nsnn_params['batch_size']=16\n",
        "    nsnn_params['steps_per_epoch']=80\n",
        "    nsnn_params['epochs']=180\n",
        "    nsnn_params['dropout']=0.7\n",
        "    nsnn_params['layer_width']=145\n",
        "    nsnn_params['n_hidden_layers']=3\n",
        "\n",
        "elif data_source == 'smart_buildings':\n",
        "    nsnn_params['sample_ratio']=25.0\n",
        "    nsnn_params['sample_delta']=0.05\n",
        "    nsnn_params['batch_size']=32\n",
        "    nsnn_params['steps_per_epoch']=16\n",
        "    nsnn_params['epochs']=88\n",
        "    nsnn_params['dropout']=0.5\n",
        "    nsnn_params['layer_width']=128\n",
        "    nsnn_params['n_hidden_layers']=2\n",
        "\n",
        "else:\n",
        "  raise InvalidDatasetError(\"You requested an invalid data set (%s).\" %data_source)\n",
        "\n",
        "print(nsnn_params)\n",
        "X_train = training_sample.drop(columns = ['class_label'])\n",
        "\n",
        "# It's very important to normalize the data for both classification and\n",
        "# anomaly detection.\n",
        "y_train = training_sample['class_label']\n",
        "\n",
        "# We shall reuse the normalization info from training for test.\n",
        "X_test = test_sample.drop(columns = ['class_label'])\n",
        "y_test= test_sample['class_label']\n",
        "\n",
        "# Declare the AD and parameterize it.\n",
        "ad = NegativeSamplingNeuralNetworkAD(\n",
        "        sample_ratio=nsnn_params['sample_ratio'],\n",
        "        sample_delta=nsnn_params['sample_delta'],\n",
        "        batch_size=nsnn_params['batch_size'],\n",
        "        steps_per_epoch=nsnn_params['steps_per_epoch'],\n",
        "        epochs=nsnn_params['epochs'],\n",
        "        dropout=nsnn_params['dropout'],\n",
        "        layer_width=nsnn_params['layer_width'],\n",
        "        n_hidden_layers=nsnn_params['n_hidden_layers'],\n",
        "        log_dir=log_dir)\n",
        "\n",
        "# Build the sample and train the AD classifier model.\n",
        "ad.train_model(x_train=X_train)\n",
        "\n",
        "xy_predicted = ad.predict(X_test)\n",
        "\n",
        "auc = madi.utils.evaluation_utils.compute_auc(\n",
        "      y_actual=y_test, y_predicted=xy_predicted['class_prob'])\n",
        "\n",
        "print(\"The AUC against the test set is %0.3f\" %auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT4BSuZ6FRJf"
      },
      "source": [
        "##3. Set up the Anomaly Interpreter\n",
        "\n",
        "*“**A key step in applying Integrated Gradients is to select\n",
        "a good baseline**”* (Sundararajan et al., 2017), so we first\n",
        "discuss how to choose a baseline, and then describe how to\n",
        "use Integrated Gradients for anomaly interpretation.\n",
        "\n",
        "\n",
        "**Proposition 3.** (Baseline Set for Anomaly Detection) *Points\n",
        "from the positive sample used to train the anomaly detection\n",
        "classifier with high Normal class confidence scores,*  $U^{*}  \\subset U : \\forall_{u \\in U^{*}} F \\left ( x \\right ) \\geq 1 - \\epsilon$ *are a sufficient baseline set.*\n",
        "\n",
        "\n",
        "\n",
        "In the original formulation of Integrated Gradients, $x$ has a high class-confidence score, which requires a baseline point that yields a near-zero class confidence score. In anomaly interpretation it is reversed; since $x$ is an anomaly, $F(x) \\approx 0$, the baseline point must be from from the Normal set, $F(u) \\approx 1$. By Assumption 1, the positive sample is sufficient and stable. A trained classifier will assign highest Normal class scores to points from regions with the greatest difference in positive and negative sample densities. Tolerance $\\epsilon$ depends on the classifier and data, but should be large enough to accumulate enough points to cover all high-confidence Normal regions. Since a uniform distribution guarantees a constant negative sample density, these highest scoring points in $U^{*}$ are in regions with the maximum density of Normal observations.\n",
        "\n",
        "Once we have established the neutral baseline set, we then determine how to apply it to any anomalous point. To simplify interpretability, we will choose the one nearest point from $U^{*}$ to anomalous point $x$ as an approximation for the closest point on the surface of the Normal volume.  Since integrated gradients computes the gradient along the straight-line path between the point and the baseline, we chose the chose the baseline point from $U^{*}$ with the minimum Euclidean distance $d$, $u^{*}=argmin_{u \\in U^{*}} \\left \\{ d \\left ( x, u \\right ) \\right \\}$.  (Using the Euclidean distance assumes a Euclidean space, which requires first normalizing all dimensions.) Along with the proportional blame provided by Integrated Gradients, we have found that the values of $u^{*}$ provide useful interpretation as expected values of the nearest normal.\n",
        "\n",
        "Now that we have a baseline point for each anomaly, we can apply the Integrated Gradients equation  (below) to assign a proportional blame, $B_{d} \\left ( x \\right )$ along the $d^{th}$ dimension for anomaly $x$ and nearest normal baseline $u^{*}$, where $\\frac{\\partial F \\left ( x \\right )}{\\partial x_{d}}$ is the gradient of the anomaly classifier function $F$, and $\\alpha$ is path variable that ranges from 0 at $x$ to 1 at $u^{*}$.\n",
        "\n",
        "$$B_{d}\\left (x \\right ) \\equiv  \\left (u_{d}^{*} - x_{d} \\right ) \\times \\int_{\\alpha=0}^{1} \\frac{\\partial F \\left ( x + \\alpha \\times \\left (u^{*}-x \\right ) \\right )}{\\partial x_{d}}d \\alpha$$\n",
        "\n",
        "The **Completeness Axiom of Integrated Gradients** ensures that each dimension is assigned a proportional blame and the sum of the blame across each dimension is bounded by tolerance $\\epsilon$ and very close to $1$: $1-\\sum_{d \\in D} B_{d} \\left ( x \\right ) \\leq \\epsilon$.\n",
        "\n",
        "If we assume the cost for computing the distance and gradient are both constant, then the run-time complexity for applying Integrated Gradients for variable attribution is linear with (a) the number of baseline points $\\left | U^{*} \\right |$, and (b) the number of $k$ steps to compute the gradient along the path variable $\\alpha$ from anomaly $x$ to the nearest baseline point $u^{*}$, $O \\left ( \\left  | U^{*} \\right | \\right ) + O \\left ( k \\right )$.\n",
        "\n",
        "Putting it all together, this approach is an anomaly detector with a novel method of interpreting the anomaly.  Propositions 1 and 2 describe a method of creating a labeled data set to train an anomaly detection classifier, and Proposition 3 proposes a way of applying Integrated Gradients and a deep network anomaly classifier to interpret an anomaly score. In the following section we will demonstrate the performance compared to other anomaly detection methods with various data sets.\n",
        "\n",
        "---\n",
        "\n",
        "Below, select the `minimum classifier confidence` $F(x)$ for the baseline set, and the maximum size of the baseline. If there are more than the maximum specifed size, we'll just choose the highest scoring points.\n",
        "\n",
        "\n",
        "If you select a value too high for the minimum classifier confidence, the baseline will be empty and will generate an error. To fix that problem, either lower the confidence threshold as specified, or go back to Step 2 and train a new model with different parameters.\n",
        "\n",
        "\n",
        "**Hint:** If the max confidence drops well below 0.95), you should train a new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4ThIBdEIWICF"
      },
      "outputs": [],
      "source": [
        "#@title Set up the anomaly interpreter.\n",
        "\n",
        "\n",
        "min_class_confidence = 0.99 #@param {type:\"number\"}\n",
        "max_baseline_size = 500 #@param {type:\"integer\"}\n",
        "\n",
        "try:\n",
        "  X_train_normalized = madi.utils.sample_utils.normalize(X_train, ad._normalization_info)\n",
        "  interpreter = madi.detectors.integrated_gradients_interpreter.IntegratedGradientsInterpreter(\n",
        "          ad._model, X_train_normalized,\n",
        "          min_class_confidence, max_baseline_size,num_steps_integrated_gradients = 2000)\n",
        "  print('Variable Attibution is ready with a baseline sample size of %d points.' %len(interpreter._df_baseline))\n",
        "except madi.detectors.integrated_gradients_interpreter.NoQualifyingBaselineError as err:\n",
        "  print(err.message)\n",
        "  print('Assign min_class_confidence to a value below %0.2f to accumulate a baseline sample.' %err.highest_class_confidence)\n",
        "  print('Variable Attibution is NOT ready.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "b-XPVtTtwtEs"
      },
      "outputs": [],
      "source": [
        "#@title Generate the sliders for interactive control.\n",
        "\n",
        "def get_variable_slider(variable_name:str, mean:float, std:float, default:float) -\u003e str:\n",
        "  if \"percent\" in variable_name:\n",
        "    min_val = 0\n",
        "    max_val = 100\n",
        "    step = 1\n",
        "  elif \"temperature\" in variable_name:\n",
        "    min_val = 250\n",
        "    max_val = 350\n",
        "    step = 1\n",
        "  elif variable_name == \"dow\":\n",
        "    min_val = 0\n",
        "    max_val = 6\n",
        "    step = 1\n",
        "  elif variable_name == \"hod\":\n",
        "    min_val = 0\n",
        "    max_val = 23\n",
        "    step = 1\n",
        "  else:\n",
        "    min_val = mean - 6 * std\n",
        "    max_val = mean + 6 * std\n",
        "    step = (max_val - min_val)/100.0\n",
        "\n",
        "  var_name_base = variable_name.replace(\"data:\", \"\")\n",
        "  return \"\"\"%s = %f #%sparam {type:\"slider\", min:%f, max:%f, step:%f}\"\"\" %(var_name_base, default, \"@\", min_val, max_val, step )\n",
        "\n",
        "def get_var_assignment(variable_name: str) -\u003e str:\n",
        "  var_name_base = variable_name.replace(\"data:\", \"\")\n",
        "  return \"\"\"observed_point['%s'] = [%s]\"\"\" %(variable_name, var_name_base)\n",
        "\n",
        "def get_sliders(normalization_info: Dict[str, madi.utils.sample_utils.Variable], baseline: pd.DataFrame):\n",
        "  column_order = madi.utils.sample_utils.get_column_order(normalization_info)\n",
        "  slider_strings  = []\n",
        "  for col in column_order:\n",
        "    slider_strings.append(get_variable_slider(col, normalization_info[col].mean, normalization_info[col].std, baseline[col]))\n",
        "  return \"\\n\".join(slider_strings)\n",
        "\n",
        "def get_var_assignments(normalization_info):\n",
        "  column_order = madi.utils.sample_utils.get_column_order(normalization_info)\n",
        "  assignment_strings  = []\n",
        "  for col in column_order:\n",
        "    assignment_strings.append(get_var_assignment(normalization_info[col].name))\n",
        "  return \"\\n\".join(assignment_strings)\n",
        "\n",
        "df_reference_point_selected_normalized = interpreter._df_baseline.iloc[[0]]\n",
        "df_reference_point_selected =madi.utils.sample_utils.denormalize(df_reference_point_selected_normalized, ad._normalization_info)\n",
        "\n",
        "print(get_sliders(ad._normalization_info,df_reference_point_selected.iloc[0]))\n",
        "print(get_var_assignments(ad._normalization_info))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIMN-h3INldE"
      },
      "source": [
        "After pasting in the code that creates the sliders, you can make modifications to observe how IG responds to univariate or multivariate anomalies.\n",
        "\n",
        "\n",
        "---\n",
        "The center of the pie chart is the normal class confidence anomaly score from the NS-NN. The surrounding slices indicate normalized attribution for each variable as percentage. The actual value is shown with the expected normal in parenthesis. Below, we show the cumulative gradients ranging from the anomalous point (left) to the nearest normal baseline point (right)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZgJ3n4KCjeqg"
      },
      "outputs": [],
      "source": [
        "#@title Visualize variable attribution. { vertical-output: false, run: \"auto\" }\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "column_order = madi.utils.sample_utils.get_column_order(ad._normalization_info)\n",
        "observed_point = {}\n",
        "\n",
        "################################################################\n",
        "#vv Copy and paste the text from the cell into the area below.vv\n",
        "################################################################\n",
        "print('If you\\'re getting an error, please open this cell and paste the output from the last cell here.')\n",
        "# Sliders should look similar to this:\n",
        "# zone_air_heating_temperature_setpoint = 285.925926 #@param {type:\"slider\", min:250.000000, max:350.000000, step:1.000000}\n",
        "# zone_air_temperature_sensor = 272 #@param {type:\"slider\", min:250.000000, max:350.000000, step:1.000000}\n",
        "# zone_air_cooling_temperature_setpoint = 267 #@param {type:\"slider\", min:250.000000, max:350.000000, step:1.000000}\n",
        "# supply_air_flowrate_sensor = -0.001365 #@param {type:\"slider\", min:-0.525120, max:0.680106, step:0.012052}\n",
        "# supply_air_damper_percentage_command = 100.000000 #@param {type:\"slider\", min:0.000000, max:100.000000, step:1.000000}\n",
        "# supply_air_flowrate_setpoint = 0.035396 #@param {type:\"slider\", min:-0.458984, max:0.618622, step:0.010776}\n",
        "# dow = 6.000000 #@param {type:\"slider\", min:0.000000, max:6.000000, step:1.000000}\n",
        "# hod = 0.000000 #@param {type:\"slider\", min:0.000000, max:23.000000, step:1.000000}\n",
        "# observed_point['data:zone_air_heating_temperature_setpoint'] = [zone_air_heating_temperature_setpoint]\n",
        "# observed_point['data:zone_air_temperature_sensor'] = [zone_air_temperature_sensor]\n",
        "# observed_point['data:zone_air_cooling_temperature_setpoint'] = [zone_air_cooling_temperature_setpoint]\n",
        "# observed_point['data:supply_air_flowrate_sensor'] = [supply_air_flowrate_sensor]\n",
        "# observed_point['data:supply_air_damper_percentage_command'] = [supply_air_damper_percentage_command]\n",
        "# observed_point['data:supply_air_flowrate_setpoint'] = [supply_air_flowrate_setpoint]\n",
        "# observed_point['dow'] = [dow]\n",
        "# observed_point['hod'] = [hod]\n",
        "################################################################\n",
        "#^^ Copy and paste the text from the cell into the area above.^^\n",
        "################################################################\n",
        "\n",
        "# Get the observed point from the sliders:\n",
        "df_observed_point = pd.DataFrame(observed_point)\n",
        "\n",
        "# Since the sliders are shown in orgininal spaces, we should normalize:\n",
        "df_observed_point_normalized = madi.utils.sample_utils.normalize(df_observed_point, ad._normalization_info)\n",
        "\n",
        "# Get the anomaly score from the NSNN.\n",
        "anomaly_score = ad.predict(df_observed_point.copy())['class_prob'][0]\n",
        "\n",
        "# Call the Blame method that applies Integrated Gradients.\n",
        "attribution_dict, reference_point_dict, df_grad = interpreter.blame(df_observed_point_normalized.iloc[0])\n",
        "attribution = pd.Series(attribution_dict)\n",
        "nearest_reference_point_normalized = pd.Series(reference_point_dict)\n",
        "df_nearest_reference_point_normalized = nearest_reference_point_normalized.to_frame().T\n",
        "df_nearest_reference_point = madi.utils.sample_utils.denormalize(df_nearest_reference_point_normalized, ad._normalization_info)\n",
        "df_attribution = pd.concat([df_observed_point.iloc[0], df_nearest_reference_point.iloc[0], attribution], axis = 1,\n",
        "                           keys=['observed_value', 'expected_value', 'attribution'], sort = True)\n",
        "\n",
        "plot_attribution(df_attribution, anomaly_score)\n",
        "\n",
        "# Plot the attribution as a cumulative curve from reference to observed.\n",
        "# Use the normalized difference, b/c the model and its gradients are based on\n",
        "# normalized training data.\n",
        "delta_normalized = df_nearest_reference_point_normalized.iloc[0] - df_observed_point_normalized.iloc[0]\n",
        "colnames  = {}\n",
        "column_order = madi.utils.sample_utils.get_column_order(ad._normalization_info)\n",
        "for i in range(len(column_order)):\n",
        "  colnames[i] = column_order[i]\n",
        "\n",
        "df_grad.set_axis(column_order, axis=1, inplace=True)\n",
        "plot_gradient_series(df_grad, delta_normalized)\n",
        "\n",
        "# TODO(sipple) from review:\n",
        "# Particularly for smart buildings, it would be cool to have the option to see\n",
        "# the attributions and the cumulative gradients for an anomaly, something like:\n",
        "# EXAMPLE_TYPE = 'reference point' #@param ['reference point', 'anomaly']\n",
        "\n",
        "# if EXAMPLE_TYPE == 'reference point':\n",
        "#   df_reference_point_selected_normalized = interpreter._df_baseline.sample(1)\n",
        "#   df_point_selected = madi.utils.sample_utils.denormalize(df_reference_point_selected_normalized, ad._normalization_info)\n",
        "# elif EXAMPLE_TYPE == 'anomaly':\n",
        "#   df_point_selected = test_sample[test_sample.class_label == 0].sample(1)\n",
        "# else:\n",
        "#   raise NotImplementedError\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Ch_2_AD_with_Interpretability.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
